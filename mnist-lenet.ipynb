{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorflow.python.platform import flags\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.model import CallableModelWrapper\n",
    "from cleverhans.utils import AccuracyReport\n",
    "from cleverhans.utils_pytorch import convert_pytorch_model_to_tf\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "class nnModel(nn.Module):\n",
    "    \"\"\" Basic MNIST model from github\n",
    "    https://github.com/rickiepark/pytorch-examples/blob/master/mnist.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(nnModel, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "\n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class aceModel_f(nn.Module):\n",
    "    \"\"\" Basic MNIST model from github\n",
    "    https://github.com/rickiepark/pytorch-examples/blob/master/mnist.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(aceModel_f, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        f = F.max_pool2d(F.relu(self.conv2(f)), 2)\n",
    "        f = f.view(-1, 64 * 7 * 7)  # reshape Variable\n",
    "        f = F.relu(self.fc1(f))\n",
    "        return f\n",
    "\n",
    "class aceModel_g(nn.Module):\n",
    "    \"\"\" Basic MNIST model from github\n",
    "    https://github.com/rickiepark/pytorch-examples/blob/master/mnist.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(aceModel_g, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)\n",
    "\n",
    "    def forward(self, y):\n",
    "        g = F.relu(self.fc1(y))\n",
    "        return g\n",
    "\n",
    "class aceModel(nn.Module):\n",
    "    \"\"\" Basic MNIST model from github\n",
    "    https://github.com/rickiepark/pytorch-examples/blob/master/mnist.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(aceModel, self).__init__()\n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "\n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def neg_hscore(f,g):\n",
    "    f0 = f - torch.mean(f,0)\n",
    "    g0 = g - torch.mean(g,0)\n",
    "    corr = torch.mean(torch.sum(f0*g0,1))\n",
    "    cov_f = torch.mm(torch.t(f0),f0) / (f0.size()[0]-1.)\n",
    "    cov_g = torch.mm(torch.t(g0),g0) / (g0.size()[0]-1.)\n",
    "    return - corr + torch.trace(torch.mm(cov_f, cov_g)) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs=1\n",
    "batch_size=128\n",
    "train_end=-1\n",
    "test_end=-1\n",
    "learning_rate=0.001\n",
    "\n",
    "model_ace = aceModel()\n",
    "model_nn = nnModel()\n",
    "model_f = aceModel_f()\n",
    "model_g = aceModel_g()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "train_loader.dataset.train_data = train_loader.dataset.train_data[:train_end]\n",
    "test_loader.dataset.test_data = test_loader.dataset.test_data[:test_end]\n",
    "\n",
    "optimizer_nn = optim.Adam(model_nn.parameters(),lr=learning_rate)\n",
    "optimizer_ace = optim.Adam(list(model_f.parameters())+list(model_g.parameters()), lr=learning_rate)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limy/anaconda3/lib/python3.6/site-packages/torch/tensor.py:263: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished.\n",
      "loss_nn:0.08271235227584839,loss_ace:-3.479642629623413\n",
      "NN test accuracy: 98.24%\n",
      "ACE test accuracy: 98.69%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    train_total = 0\n",
    "    correct = 0\n",
    "    step = 0\n",
    "    py = np.zeros((1,10))\n",
    "    loss_nn_total=0\n",
    "    for xs, ys in train_loader:\n",
    "        ys_1hot = torch.zeros(len(ys), 10).scatter_(1, ys.resize(len(ys),1), 1)\n",
    "        # print(ys,ys.size(),ys.type())\n",
    "        xs, ys = Variable(xs), Variable(ys)\n",
    "        if torch.cuda.is_available():\n",
    "            xs, ys ,ys_1hot = xs.cuda(), ys.cuda(), ys_1hot.cuda()\n",
    "        optimizer_nn.zero_grad()\n",
    "        optimizer_ace.zero_grad()\n",
    "        logits_nn = model_nn(xs)\n",
    "        # pred = torch.max(logits_nn,1)[1]\n",
    "        # acc = (pred==ys).sum()\n",
    "        # print(xs[-1])\n",
    "        f = model_f(xs)\n",
    "        g = model_g(ys_1hot)\n",
    "        loss_ace = neg_hscore(f,g)\n",
    "        loss_ace.backward()\n",
    "        loss_nn = F.cross_entropy(logits_nn, ys)\n",
    "        loss_nn.backward()  # calc gradients\n",
    "        # print(loss_nn)\n",
    "        optimizer_nn.step()\n",
    "        optimizer_ace.step()  # update gradients\n",
    "        py = py + torch.sum(ys_1hot,0).cpu().numpy()\n",
    "        train_total += len(xs)\n",
    "    print('Epoch {} finished.'.format(epoch))\n",
    "    print(\"loss_nn:{},loss_ace:{}\".format(loss_nn,loss_ace))\n",
    "    py = py.astype(float) / train_total\n",
    "    # Evaluate on clean data\n",
    "    total = 0\n",
    "    correct_ace = 0\n",
    "    correct_nn = 0\n",
    "    if torch.cuda.is_available():\n",
    "        eye = torch.eye(10).cuda()\n",
    "    else:\n",
    "        eye = torch.eye(10)\n",
    "    g_test = model_g(eye).data.cpu().numpy()\n",
    "    g_test = g_test - np.mean(g_test, axis = 0)\n",
    "    for xs, ys in test_loader:\n",
    "        xs, ys = Variable(xs), Variable(ys)\n",
    "        if torch.cuda.is_available():\n",
    "            xs, ys = xs.cuda(), ys.cuda()\n",
    "\n",
    "        logits_nn = model_nn(xs).data.cpu().numpy()\n",
    "        # logits_nn_np = logits_nn.data.cpu().numpy()\n",
    "        f_test = model_f(xs).data.cpu().numpy()\n",
    "        f_test = f_test - np.mean(f_test, axis = 0)\n",
    "\n",
    "        # py = np.mean(y_train, axis = 0)\n",
    "        pygx = py * (1 + np.matmul(f_test, g_test.T))\n",
    "        # ace_acc = np.mean(np.argmax(pygx, axis = 1) == np.argmax(y_test, axis = 1))\n",
    "\n",
    "        correct_ace += (np.argmax(pygx, axis = 1) == ys).sum()\n",
    "        correct_nn += (np.argmax(logits_nn, axis=1) == ys).sum()\n",
    "        total += len(xs)\n",
    "\n",
    "    nn_acc = float(correct_nn) / total\n",
    "    ace_acc = float(correct_ace) / total\n",
    "\n",
    "    print('NN test accuracy: %.2f%%' % (nn_acc * 100))\n",
    "    print('ACE test accuracy: %.2f%%' % (ace_acc * 100))\n",
    "\n",
    "# print(\"py:{},pygx:{}\".format(py,pygx))\n",
    "model_f_dict = model_f.state_dict()\n",
    "model_ace_dict = model_ace.state_dict()\n",
    "model_f_dict = {key: value for key, value in model_f_dict.items() if key in model_ace_dict}\n",
    "if torch.cuda.is_available():\n",
    "    model_f_dict['fc2.weight'] = torch.from_numpy((py * g_test.T).T).cuda()\n",
    "    model_f_dict['fc2.bias'] = torch.from_numpy(py).view(10).cuda()\n",
    "else:\n",
    "    model_f_dict['fc2.weight'] = torch.from_numpy((py * g_test.T).T)\n",
    "    model_f_dict['fc2.bias'] = torch.from_numpy(py).view(10)           \n",
    "model_ace_dict.update(model_f_dict)\n",
    "model_ace.load_state_dict(model_f_dict)\n",
    "\n",
    "torch.save(model_nn.state_dict(), 'model_nn_cpu_pytorch_{}epochs.pkl'.format(nb_epochs))\n",
    "torch.save(model_ace.state_dict(), 'model_ace_cpu_pytorch_{}epochs.pkl'.format(nb_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training Start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limy/anaconda3/lib/python3.6/site-packages/torch/tensor.py:263: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished.\n",
      "Adversarial Training Finish.\n"
     ]
    }
   ],
   "source": [
    "print(\"Adversarial Training Start.\")\n",
    "eps=0.2\n",
    "epochs=1\n",
    "for epoch in range(epochs): \n",
    "    for xs,ys in train_loader:\n",
    "        ys_1hot = torch.zeros(len(ys), 10).scatter_(1, ys.resize(len(ys),1), 1)\n",
    "\n",
    "        x_nn, y_nn = Variable(xs, requires_grad=True), Variable(ys)\n",
    "        x_ace, y_ace, y_1hot_ace = Variable(xs, requires_grad=True), Variable(ys), Variable(ys_1hot)\n",
    "\n",
    "        zero_gradients(x_nn)\n",
    "        out = model_nn(x_nn)\n",
    "        _loss = F.cross_entropy(out, y_nn)\n",
    "        _loss.backward()\n",
    "        normed_grad = eps * torch.sign(x_nn.grad.data)\n",
    "        nn_adv = x_nn.data + normed_grad\n",
    "        nn_adv = torch.clamp(nn_adv, 0.0, 1.0)\n",
    "\n",
    "        zero_gradients(x_ace)\n",
    "        f = model_f(x_ace)\n",
    "        g = model_g(y_1hot_ace)\n",
    "        _loss_ace = neg_hscore(f,g)\n",
    "        _loss_ace.backward()\n",
    "        normed_grad = eps * torch.sign(x_ace.grad.data)\n",
    "        ace_adv = x_ace.data + normed_grad\n",
    "        ace_adv = torch.clamp(ace_adv, 0.0, 1.0)\n",
    "\n",
    "        optimizer_nn.zero_grad()\n",
    "        logits_nn = model_nn(nn_adv)\n",
    "        loss_nn = F.cross_entropy(logits_nn, y_nn)\n",
    "        loss_nn.backward()  # calc gradients\n",
    "        optimizer_nn.step()\n",
    "\n",
    "        optimizer_ace.zero_grad()\n",
    "        f = model_f(ace_adv)\n",
    "        g = model_g(ys_1hot)\n",
    "        loss_ace = neg_hscore(f,g)\n",
    "        loss_ace.backward()\n",
    "        optimizer_ace.step()  # update gradients\n",
    "    print(\"Epoch {} finished.\".format(epoch+1))\n",
    "    \n",
    "g_test = model_g(torch.eye(10)).data.cpu().numpy()\n",
    "g_test = g_test - np.mean(g_test, axis = 0)\n",
    "model_f_dict = model_f.state_dict()\n",
    "model_ace_dict = model_ace.state_dict()\n",
    "model_f_dict = {key: value for key, value in model_f_dict.items() if key in model_ace_dict}\n",
    "if torch.cuda.is_available():\n",
    "    model_f_dict['fc2.weight'] = torch.from_numpy((py * g_test.T).T).cuda()\n",
    "    model_f_dict['fc2.bias'] = torch.from_numpy(py).view(10).cuda()\n",
    "else:\n",
    "    model_f_dict['fc2.weight'] = torch.from_numpy((py * g_test.T).T)\n",
    "    model_f_dict['fc2.bias'] = torch.from_numpy(py).view(10)           \n",
    "model_ace_dict.update(model_f_dict)\n",
    "model_ace.load_state_dict(model_f_dict)\n",
    "print(\"Adversarial Training Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps:0.05\n",
      "nn Adv accuracy: 97.680\n",
      "nn_ori Adv accuracy: 94.589\n",
      "ace Adv accuracy: 97.350\n",
      "ace_ori Adv accuracy: 96.080\n",
      "eps:0.1\n",
      "nn Adv accuracy: 96.120\n",
      "nn_ori Adv accuracy: 80.658\n",
      "ace Adv accuracy: 97.540\n",
      "ace_ori Adv accuracy: 87.719\n",
      "eps:0.15000000000000002\n",
      "nn Adv accuracy: 93.379\n",
      "nn_ori Adv accuracy: 53.185\n",
      "ace Adv accuracy: 97.090\n",
      "ace_ori Adv accuracy: 69.417\n",
      "eps:0.2\n",
      "nn Adv accuracy: 89.439\n",
      "nn_ori Adv accuracy: 25.463\n",
      "ace Adv accuracy: 96.150\n",
      "ace_ori Adv accuracy: 37.774\n",
      "eps:0.25\n",
      "nn Adv accuracy: 82.528\n",
      "nn_ori Adv accuracy: 8.781\n",
      "ace Adv accuracy: 94.599\n",
      "ace_ori Adv accuracy: 9.201\n",
      "eps:0.3\n",
      "nn Adv accuracy: 72.307\n",
      "nn_ori Adv accuracy: 3.130\n",
      "ace Adv accuracy: 92.399\n",
      "ace_ori Adv accuracy: 3.990\n",
      "eps:0.35000000000000003\n",
      "nn Adv accuracy: 56.886\n",
      "nn_ori Adv accuracy: 1.600\n",
      "ace Adv accuracy: 89.449\n",
      "ace_ori Adv accuracy: 3.150\n",
      "eps:0.4\n",
      "nn Adv accuracy: 38.574\n",
      "nn_ori Adv accuracy: 1.130\n",
      "ace Adv accuracy: 85.359\n",
      "ace_ori Adv accuracy: 2.980\n"
     ]
    }
   ],
   "source": [
    "model_ace_ori = aceModel()\n",
    "model_ace_ori.load_state_dict(torch.load('model_ace_cpu_pytorch_1epochs.pkl'))\n",
    "model_nn_ori = nnModel()\n",
    "model_nn_ori.load_state_dict(torch.load('model_nn_cpu_pytorch_1epochs.pkl'))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "x_op = tf.placeholder(tf.float32, shape=(None, 1, 28, 28,))\n",
    "\n",
    "# Convert pytorch model to a tf_model and wrap it in cleverhans\n",
    "tf_model_nn = convert_pytorch_model_to_tf(model_nn)\n",
    "tf_model_ace = convert_pytorch_model_to_tf(model_ace)\n",
    "tf_model_nn_ori = convert_pytorch_model_to_tf(model_nn_ori)\n",
    "tf_model_ace_ori = convert_pytorch_model_to_tf(model_ace_ori)\n",
    "cleverhans_model_nn = CallableModelWrapper(tf_model_nn, output_layer='logits')\n",
    "cleverhans_model_ace = CallableModelWrapper(tf_model_ace, output_layer='logits')\n",
    "cleverhans_model_nn_ori = CallableModelWrapper(tf_model_nn_ori, output_layer='logits')\n",
    "cleverhans_model_ace_ori = CallableModelWrapper(tf_model_ace_ori, output_layer='logits')\n",
    "\n",
    "# Create an FGSM attack\n",
    "for eps in np.arange(0.05,0.45,0.05):\n",
    "\n",
    "    fgsm_params = {'eps': eps,\n",
    "                   'clip_min': 0.,\n",
    "                   'clip_max': 1.}\n",
    "\n",
    "    fgsm_nn = FastGradientMethod(cleverhans_model_nn, sess=sess)\n",
    "    adv_x_nn = fgsm_nn.generate(x_op, **fgsm_params)\n",
    "    adv_pred_nn = tf_model_nn(adv_x_nn)\n",
    "\n",
    "    fgsm_ace = FastGradientMethod(cleverhans_model_ace, sess=sess)\n",
    "    adv_x_ace = fgsm_ace.generate(x_op, **fgsm_params)\n",
    "    adv_pred_ace = tf_model_ace(adv_x_ace)\n",
    "\n",
    "    fgsm_nn_ori = FastGradientMethod(cleverhans_model_nn_ori, sess=sess)\n",
    "    adv_x_nn_ori = fgsm_nn_ori.generate(x_op, **fgsm_params)\n",
    "    adv_pred_nn_ori = tf_model_nn_ori(adv_x_nn_ori)\n",
    "\n",
    "    fgsm_ace_ori = FastGradientMethod(cleverhans_model_ace_ori, sess=sess)\n",
    "    adv_x_ace_ori = fgsm_ace_ori.generate(x_op, **fgsm_params)\n",
    "    adv_pred_ace_ori = tf_model_ace_ori(adv_x_ace_ori)\n",
    "    # Run an evaluation of our model against fgsm\n",
    "    total = 0\n",
    "    correct_nn = 0\n",
    "    correct_nn_ori = 0\n",
    "    correct_ace = 0\n",
    "    correct_ace_ori = 0\n",
    "    for xs, ys in test_loader:\n",
    "        adv_xs_nn, adv_preds_nn = sess.run([adv_x_nn, adv_pred_nn] , feed_dict={x_op: xs})\n",
    "        adv_xs_nn__ori, adv_preds_nn_ori = sess.run([adv_x_nn_ori, adv_pred_nn_ori] , feed_dict={x_op: xs})\n",
    "        adv_xs_ace, adv_preds_ace = sess.run([adv_x_ace, adv_pred_ace], feed_dict={x_op: xs})\n",
    "        adv_xs_ace_ori, adv_preds_ace_ori = sess.run([adv_x_ace_ori, adv_pred_ace_ori], feed_dict={x_op: xs})\n",
    "        # print(xs)\n",
    "        # print(x_op[-1])\n",
    "        # print(np.amax(adv_xs_nn[-1]),np.amin(adv_xs_nn[-1]))\n",
    "        correct_nn += (np.argmax(adv_preds_nn, axis=1) == ys).sum()\n",
    "        correct_nn_ori += (np.argmax(adv_preds_nn_ori, axis=1) == ys).sum()\n",
    "        correct_ace += (np.argmax(adv_preds_ace, axis=1) == ys).sum()\n",
    "        correct_ace_ori += (np.argmax(adv_preds_ace_ori, axis=1) == ys).sum()\n",
    "        total += len(xs)\n",
    "\n",
    "    acc_nn = float(correct_nn) / total\n",
    "    acc_nn_ori = float(correct_nn_ori) / total\n",
    "    acc_ace = float(correct_ace) / total\n",
    "    acc_ace_ori = float(correct_ace_ori) / total\n",
    "    print('eps:{}'.format(eps))\n",
    "    print('nn Adv accuracy: {:.3f}'.format(acc_nn * 100))\n",
    "    print('nn_ori Adv accuracy: {:.3f}'.format(acc_nn_ori * 100))\n",
    "    print('ace Adv accuracy: {:.3f}'.format(acc_ace * 100))\n",
    "    print('ace_ori Adv accuracy: {:.3f}'.format(acc_ace_ori * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_nn.state_dict(), 'model_adv_nn_cpu_pytorch_1epochs.pkl')\n",
    "torch.save(model_ace.state_dict(), 'model_adv_ace_cpu_pytorch_1epochs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps:0.05\n",
      "nn Adv accuracy: 97.680\n",
      "nn_ori Adv accuracy: 94.589\n",
      "ace Adv accuracy: 97.350\n",
      "ace_ori Adv accuracy: 96.080\n",
      "eps:0.1\n",
      "nn Adv accuracy: 96.120\n",
      "nn_ori Adv accuracy: 80.658\n",
      "ace Adv accuracy: 97.540\n",
      "ace_ori Adv accuracy: 87.719\n",
      "eps:0.15000000000000002\n",
      "nn Adv accuracy: 93.379\n",
      "nn_ori Adv accuracy: 53.185\n",
      "ace Adv accuracy: 97.090\n",
      "ace_ori Adv accuracy: 69.417\n",
      "eps:0.2\n",
      "nn Adv accuracy: 89.439\n",
      "nn_ori Adv accuracy: 25.463\n",
      "ace Adv accuracy: 96.150\n",
      "ace_ori Adv accuracy: 37.774\n",
      "eps:0.25\n",
      "nn Adv accuracy: 82.528\n",
      "nn_ori Adv accuracy: 8.781\n",
      "ace Adv accuracy: 94.599\n",
      "ace_ori Adv accuracy: 9.201\n",
      "eps:0.3\n",
      "nn Adv accuracy: 72.307\n",
      "nn_ori Adv accuracy: 3.130\n",
      "ace Adv accuracy: 92.399\n",
      "ace_ori Adv accuracy: 3.990\n",
      "eps:0.35000000000000003\n",
      "nn Adv accuracy: 56.886\n",
      "nn_ori Adv accuracy: 1.600\n",
      "ace Adv accuracy: 89.449\n",
      "ace_ori Adv accuracy: 3.150\n",
      "eps:0.4\n",
      "nn Adv accuracy: 38.574\n",
      "nn_ori Adv accuracy: 1.130\n",
      "ace Adv accuracy: 85.359\n",
      "ace_ori Adv accuracy: 2.980\n"
     ]
    }
   ],
   "source": [
    "model_ace = aceModel()\n",
    "model_ace.load_state_dict(torch.load('model_adv_ace_cpu_pytorch_1epochs.pkl'))\n",
    "model_nn = nnModel()\n",
    "model_nn.load_state_dict(torch.load('model_adv_nn_cpu_pytorch_1epochs.pkl'))\n",
    "model_ace_ori = aceModel()\n",
    "model_ace_ori.load_state_dict(torch.load('model_ace_cpu_pytorch_1epochs.pkl'))\n",
    "model_nn_ori = nnModel()\n",
    "model_nn_ori.load_state_dict(torch.load('model_nn_cpu_pytorch_1epochs.pkl'))\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "x_op = tf.placeholder(tf.float32, shape=(None, 1, 28, 28,))\n",
    "\n",
    "# Convert pytorch model to a tf_model and wrap it in cleverhans\n",
    "tf_model_nn = convert_pytorch_model_to_tf(model_nn)\n",
    "tf_model_ace = convert_pytorch_model_to_tf(model_ace)\n",
    "tf_model_nn_ori = convert_pytorch_model_to_tf(model_nn_ori)\n",
    "tf_model_ace_ori = convert_pytorch_model_to_tf(model_ace_ori)\n",
    "cleverhans_model_nn = CallableModelWrapper(tf_model_nn, output_layer='logits')\n",
    "cleverhans_model_ace = CallableModelWrapper(tf_model_ace, output_layer='logits')\n",
    "cleverhans_model_nn_ori = CallableModelWrapper(tf_model_nn_ori, output_layer='logits')\n",
    "cleverhans_model_ace_ori = CallableModelWrapper(tf_model_ace_ori, output_layer='logits')\n",
    "\n",
    "# Create an FGSM attack\n",
    "for eps in np.arange(0.05,0.45,0.05):\n",
    "\n",
    "    fgsm_params = {'eps': eps,\n",
    "                   'clip_min': 0.,\n",
    "                   'clip_max': 1.}\n",
    "\n",
    "    fgsm_nn = FastGradientMethod(cleverhans_model_nn, sess=sess)\n",
    "    adv_x_nn = fgsm_nn.generate(x_op, **fgsm_params)\n",
    "    adv_pred_nn = tf_model_nn(adv_x_nn)\n",
    "\n",
    "    fgsm_ace = FastGradientMethod(cleverhans_model_ace, sess=sess)\n",
    "    adv_x_ace = fgsm_ace.generate(x_op, **fgsm_params)\n",
    "    adv_pred_ace = tf_model_ace(adv_x_ace)\n",
    "\n",
    "    fgsm_nn_ori = FastGradientMethod(cleverhans_model_nn_ori, sess=sess)\n",
    "    adv_x_nn_ori = fgsm_nn_ori.generate(x_op, **fgsm_params)\n",
    "    adv_pred_nn_ori = tf_model_nn_ori(adv_x_nn_ori)\n",
    "\n",
    "    fgsm_ace_ori = FastGradientMethod(cleverhans_model_ace_ori, sess=sess)\n",
    "    adv_x_ace_ori = fgsm_ace_ori.generate(x_op, **fgsm_params)\n",
    "    adv_pred_ace_ori = tf_model_ace_ori(adv_x_ace_ori)\n",
    "    # Run an evaluation of our model against fgsm\n",
    "    total = 0\n",
    "    correct_nn = 0\n",
    "    correct_nn_ori = 0\n",
    "    correct_ace = 0\n",
    "    correct_ace_ori = 0\n",
    "    for xs, ys in test_loader:\n",
    "        adv_xs_nn, adv_preds_nn = sess.run([adv_x_nn, adv_pred_nn] , feed_dict={x_op: xs})\n",
    "        adv_xs_nn__ori, adv_preds_nn_ori = sess.run([adv_x_nn_ori, adv_pred_nn_ori] , feed_dict={x_op: xs})\n",
    "        adv_xs_ace, adv_preds_ace = sess.run([adv_x_ace, adv_pred_ace], feed_dict={x_op: xs})\n",
    "        adv_xs_ace_ori, adv_preds_ace_ori = sess.run([adv_x_ace_ori, adv_pred_ace_ori], feed_dict={x_op: xs})\n",
    "        # print(xs)\n",
    "        # print(x_op[-1])\n",
    "        # print(np.amax(adv_xs_nn[-1]),np.amin(adv_xs_nn[-1]))\n",
    "        correct_nn += (np.argmax(adv_preds_nn, axis=1) == ys).sum()\n",
    "        correct_nn_ori += (np.argmax(adv_preds_nn_ori, axis=1) == ys).sum()\n",
    "        correct_ace += (np.argmax(adv_preds_ace, axis=1) == ys).sum()\n",
    "        correct_ace_ori += (np.argmax(adv_preds_ace_ori, axis=1) == ys).sum()\n",
    "        total += len(xs)\n",
    "\n",
    "    acc_nn = float(correct_nn) / total\n",
    "    acc_nn_ori = float(correct_nn_ori) / total\n",
    "    acc_ace = float(correct_ace) / total\n",
    "    acc_ace_ori = float(correct_ace_ori) / total\n",
    "    print('eps:{}'.format(eps))\n",
    "    print('nn Adv accuracy: {:.3f}'.format(acc_nn * 100))\n",
    "    print('nn_ori Adv accuracy: {:.3f}'.format(acc_nn_ori * 100))\n",
    "    print('ace Adv accuracy: {:.3f}'.format(acc_ace * 100))\n",
    "    print('ace_ori Adv accuracy: {:.3f}'.format(acc_ace_ori * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
